---
title: "Introducing UTF-21, a toy character encoding"
description: "I created UTF-21, an impractical alternative to character encodings like UTF-8."
url: /utf-21
date: 2023-06-09
---

In short: I created UTF-21, an impractical alternative to character encodings like UTF-8.

## Quick crash course: character encoding & Unicode

Before you can understand my horrible creation, you need to understand a little about Unicode. You can [skip this](#introducing-utf-21) if you want.

### Each character has a number

Character encoding is the process of converting characters to numbers and back, typically for digital storage and transmission.

You've probably heard of [ASCII], which maps 128 characters to numbers. For example, `W` is number 87 and number 36 is `$`.

As you might expect, there are more than 128 characters in the world. Characters like `√±` and `ü•∫` can't be represented as ASCII.

[Unicode] is like ASCII, but instead of 128 characters, there are 1,114,111 characters. Way more! That lets us store characters like `√±` (character #241) and `ü•∫` (character #129402). It's a little more complex than this, but that's the rough idea.

Here are a few examples from the big Unicode table:

| Character | Unicode scalar |
| --------- | -------------- |
| `F`       | `70`           |
| `√±`       | `241`          |
| `ü•∫`      | `129402`       |

(Note that some glyphs, like `üë©üèæ‚Äçüåæ`, are made up of _multiple_ characters and therefore have multiple scalars. For more, see [this post][1].)

If you want to represent this full range‚Äî0 to ~1.1 million‚Äîyou need 21 bits of data. How do people store these bits?

### Storing the numbers

[Unicode has three official ways of storing these numbers](https://www.unicode.org/standard/principles.html): UTF-8, UTF-16, and UTF-32.

I think UTF-32 is the simplest. Each number is put into a 32-bit integer, or 4 bytes. This is called a "fixed-width" encoding. Because you only need 21 bits of data, more than a third of the space is wasted, but it's simpler and faster for some operations.

In constrast, UTF-8 and UTF-16 are "variable-width" encodings. UTF-16 tries to fit characters into a single 16-bit number, and if it can't, it expands to two. UTF-8 is conceptually similar, but it uses 8-bit numbers (bytes) as the smallest unit. (Fun fact: UTF-8 is a superset of ASCII.)

For example, for the character `F`, which has a scalar value of `70` (`46` in hex):

<table>
    <thead>
        <tr>
            <th>Encoding</th>
            <th>Bytes</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <th>UTF-32</th>
            <td><code>00 00 00 46</code></td>
        </tr>
        <tr>
            <th>UTF-16</th>
            <td><code>00 46</code></td>
        </tr>
        <tr>
            <th>UTF-8</th>
            <td><code>46</code></td>
        </tr>
    </thead>
</table>

And for the character `ü•∫`, which has a scalar value of `129402` (`01f97a` in hex):

<table>
    <thead>
        <tr>
            <th>Encoding</th>
            <th>Bytes</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <th>UTF-32</th>
            <td><code>00 01 f9 7a</code></td>
        </tr>
        <tr>
            <th>UTF-16</th>
            <td><code>d8 3e dd 7a</code></td>
        </tr>
        <tr>
            <th>UTF-8</th>
            <td><code>f0 9f a5 ba</code></td>
        </tr>
    </thead>
</table>

## Introducing UTF-21

UTF-8, UTF-16, and UTF-32 are widely used and a lot of smart people have worked on them.

Today, I'm introducing **UTF-21**, a toy character encoding made by me, a [lone] dingus.

To represent the full range of Unicode scalars, you need 21 bits. That's precisely what UTF-21 does. Each scalar is represented by a 21-bit number, packed back-to-back with no space between.

For example, `F` has a scalar value of `70`, which is encoded like this in binary:

     000000000000001000110

`ü•∫`, which has a scalar value of `129402`, is encoded like this:

     000011111100101111010

Modern computers like to store data as _bytes_, not bits. Therefore, the end of the data is padded with zeroes until it fits in a byte. That means there will be between 0 and 7 bits of padding at the end of a UTF-21 data stream.

The string `Fü•∫` would be encoded like this in binary:

     000000000000001000110 000011111100101111010 000000

The first 21 bits are for the `F`, the next 21 are for `ü•∫`, and the last 6 are padding.

<noscript>
    <p>If you want to try it out, I built <a href="https://git.sr.ht/~evanhahn/UTF-21.js" target="_blank" rel="noopener noreferrer">UTF-21.js</a>, a JavaScript library to encode and decode UTF-21 strings.</p>
</noscript>

<div id="utf21-demo"></div>
{{< javascript "js/utf-21/index.js" >}}

## How does it perform?

UTF-21 is just a toy project, but how does it stack up against the official UTFs?

- UTF-32: UTF-21 is always more efficient than UTF-32 because it's 11 bits smaller per character. You'd expect UTF-21 strings to be roughly 66% the size of their UTF-32 counterparts.

  For example, `Hello world!` is 32 bytes in UTF-21 and 48 bytes in UTF-32.

- UTF-16: UTF-21 is less efficient than UTF-16 when you don't need two code units (surrogate pairs) and more efficient when you do. It depends on your use case, but I suspect most strings will be more efficient in UTF-16.

  For example, UTF-16 wins for the string `foo bar`: 14 bytes of UTF-16 and 19 bytes of UTF-21. But it loses for the string `üåçüåèüåé`: 12 bytes of UTF-16 and only 8 for UTF-21.

- UTF-8: UTF-21 is less efficient than UTF-8 unless you need three or four bytes for the character, and then UTF-21 is more efficient.

  For example, UTF-8 wins for the string `foo bar`‚Äî7 bytes versus 19‚Äîbut loses for the string `ÏïàÎÖïÌïòÏÑ∏Ïöî`‚Äî14 bytes to 15.

In short, it's more efficient than UTF-32 but probably worse than the others in most cases.

## Why did I do this?

For fun!

UTF-21 probably goes under the category of ["useless stuff"][0]. It's not particularly efficient or good, but I learned a bunch about how Unicode works and had a lot of fun building it.

I hope this was equally fun and informative to read!

_Thanks to [Manuel Strehl] for reviewing an early draft of this post._

[lone]: {{< relref "posts/2023-02-27-the-lone-developer-problem" >}}
[ASCII]: https://man7.org/linux/man-pages/man7/ascii.7.html
[Unicode]: https://unicode.org
[UTF-21.js]: https://git.sr.ht/~evanhahn/UTF-21.js
[0]: https://austinhenley.com/blog/makinguselessstuff.html
[1]: https://hsivonen.fi/string-length/
[Manuel Strehl]: https://codepoints.net
